\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

    \title{Multi-head Attention Pruning in Decoder Transformers}

    \author{\IEEEauthorblockN{Welby Seely}
    \IEEEauthorblockA{\textit{Embedded AI Systems Lab} \\
    \textit{Eastern Michigan University}\\
    Ypsilanti, United States of America\\
    wseely@emich.edu}}

    \maketitle

    \begin{abstract}
        This paper introduces a framework for systematically pruning multi-head attention in decoder-based Large Language Models (LLMs). Unlike existing approaches, this framework provides holistic analysis capabilities by integrating multiple evaluation metrics such as perplexity, energy consumption, evaluation time, and memory usage. The methodology focuses on the Llama 3 architecture, performing layer-wise pruning and sensitivity analysis to determine the effects of approximating its attention mechanism. Results demonstrate potential savings in computational resources and highlight specific layers as candidates for approximation. Future work aims to extend this framework to additional components and model architectures.
    \end{abstract}

    \begin{IEEEkeywords}
        Large Language Models (LLMs), attention mechanism, pruning, resource optimization, sensitivity analysis
    \end{IEEEkeywords}

    \section{Introduction}
    Recent advancements in Large Language Models (LLMs) have achieved state-of-the-art performance across numerous natural language processing tasks. However, these improvements come at the cost of significant computational and energy resources, prompting the need for efficient approximation techniques such as pruning. This work focuses on multi-head attention pruning in decoder transformers, introducing a structured framework for evaluating the impact of pruning on various model components.

    This research addresses limitations in the current literature, which often narrowly focus on specific metrics like perplexity and lack a unified framework for holistic analysis. By leveraging a multi-metric approach, this work explores the systemic effects of pruning, aiming to establish a more comprehensive understanding of parameter sensitivity in LLMs.

    \section{Motivation and Background}
    The initial goal of this study was to understand methodologies for determining neuron sensitivity in LLMs, revealed through a literature review to be fragmented and narrowly scoped. Existing studies often focus on specific optimizations, such as vertical parameter pruning, but fail to account for broader metrics like energy consumption or evaluation time.

    Key limitations in the reviewed literature include:
    \begin{itemize}
        \item Inability to prune arbitrary heads.
        \item Lack of per-token execution time and energy consumption measurements.
        \item Limited support for models using Grouped Query Attention.
        \item Narrow focus on singular types of optimizations.
    \end{itemize}
    These limitations underscore the need for a holistic framework that supports systematic analysis of LLM components.

    \section{Proposed Framework}
    This work proposes a structured framework targeting the Llama 3 decoder architecture. The framework evaluates the effects of multi-head attention pruning across four key metrics:
    \begin{enumerate}
        \item Perplexity
        \item Energy consumption per token
        \item Evaluation time per token
        \item Allocated memory usage
    \end{enumerate}

    The pruning methodology involves layer-wise removal of attention heads, including associated key and value heads orphaned by pruning. Sensitivity scores are calculated by comparing the performance of pruned layers against an unpruned baseline, allowing for a detailed analysis of which layers and components are more resilient to pruning.

    \section{Experimental Setup}
    The framework was tested on the Llama 3 8b model, consisting of 32 decoder layers and 32 attention heads per layer. The dataset used for evaluation was wikitext-2-v1. Each layer was assessed with a batch size of 5 across 25 prompts, running on an RTX 4090 GPU with the desktop environment disabled.

    Four metrics were collected for each pruned layer and compared to the unpruned baseline:
    \begin{itemize}
        \item Perplexity
        \item Energy consumption per token
        \item Evaluation time per token
        \item Allocated memory usage
    \end{itemize}

    \section{Results and Discussion}
    \subsection{Perplexity}
    Perplexity measurements revealed that early layers were the most sensitive to pruning, with the first layer exhibiting over a 25\% degradation in perplexity compared to the baseline. Layers 20 through 28 were identified as good candidates for approximation due to their lower sensitivity.

    \subsection{Energy Consumption and Evaluation Time}
    Energy consumption per token decreased progressively with layer index, aligning with trends in evaluation time per token. This correlation suggests that later layers may offer significant energy and time savings when approximated.

    \subsection{Memory Usage}
    Allocated memory usage remained consistent across all layers, with negligible fluctuations attributed to noise. This indicates that the framework's pruning strategy maintains parameter efficiency.

    \section{Future Work}
    Future work includes enabling head pruning based on different strategies, measuring progressive pruning effects, generating heatmaps for head-level sensitivity, and expanding the framework to support additional components and architectures. This will provide deeper insights into the interactions between components and allow for broader applicability.

    \section*{Acknowledgment}
    The author would like to thank the Embedded AI Systems Lab for their support and resources in conducting this study.

    \begin{thebibliography}{00}
        \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
        \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
        \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
    \end{thebibliography}

\end{document}
