% Feedback:
% Start actually implementing (or even better try to find open-source implementations) models
% How do these works evaluate their energy savings (if they do)? Do they evaluate actual hardware/simulated hardware?

\subsubsection{Input Sensitivity Analysis}\label{subsubsec:individual-neuron-importance-analysis}

This is an analysis of optimization techniques with importance measurements based on measuring the sensitivity of a neuron or group of neurons to the DNNs original input features. \@.
\begin{itemize}
    \item Activation Maximization ~\cite{erhan2009visualizing}
    \subitem Optimize an input to maximize the activation of a specific neuron or neuron layer.
    \item Saliency Maps
    \subitem Visualize Neuron sensitivity by highlighting inputs that most affect a particular neuron or output~\cite{hsu2023explainable}.
    \item Layer-wise Relevance Propagation
    \subitem Similar to Saliency Maps, decomposes the prediction of a network back to the individual input features~\cite{jia2022interpreting}.
    \item Integrated Gradients
    \subitem Quantifies feature importance by integrating gradients along the path from a baseline input to the actual input~\cite{sundararajan2017axiomatic}.
    \item SHAP Values (SHapley Additive exPlanation)
    \subitem Quantifies sensitivity by assigning a contribution score to each input feature based on its impact on the modelâ€™s output~\cite{nohara2022explanation}.
\end{itemize}