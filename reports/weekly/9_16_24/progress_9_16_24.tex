\documentclass{article}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{graphicx}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
\usepackage{listings}
\usepackage{xcolor}

\lstset{
    language=Python,
    basicstyle=\ttfamily\footnotesize\selectfont, % use the selected monospaced font
    backgroundcolor=\color{white},
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=10pt,
    frame=single,
    breaklines=true,
    captionpos=b,
    tabsize=4
}

\title{Progress Report - Week 1 \\
\large Novel Methods for Determining Neuron Sensitivity in Large Language Models}
\author{
    [Welby Seely] \\
    \texttt{[wseely@emich.edu]}
}
\date{\today}

\begin{document}

    \maketitle

    \section*{Project Overview}
    Large Language Model (LLM) power consumption is increasingly problematic for datacenters, with \textit{The Electric Power
    Research Institute} forecasting that data centers may see their electricity consumption double by
    2030~\cite{kindig2024}.

    Optimizing these models is key to reducing power consumption.
    The cornerstone of optimization is determining what you need, and what you don't: determine the importance of
    aspects and components of the system, enabling you to perform systematic optimization with certainty.
    In Artificial Neural Networks (ANN), the atomic unit of these components is the artificial neuron.
    To optimize an ANN, understanding the sensitivity of the neuron to changes in its input and parameters is paramount,
    but the sheer size and complexity of LLMs makes this incredibly challenging.

    Thus, the goal of this research is to:
    \begin{enumerate}
        \item Understand current methodologies for determining neuron sensitivity in LLMs.
        \item Explore improvements that can be made to more accurately and more efficiently determine neuron sensitivity in
        LLMs.
    \end{enumerate}

    The intent is that this research will be a first step towards innovating and improving optimizations for LLMs,
    reducing power consumption and enabling the models to run on lower powered hardware.

    \section*{Current Progress}
    \subsection*{Literature Review}
    Initial exploration of the literature regarding neuron sensitivity and component importance has revealed the
    following techniques:

    \begin{itemize}
        \item Coupled Structure Analysis
        \subitem Identifies dependency-based structures, in the context of pruning~\cite{ma2023llm}.
        \item Activation Maximization
        \subitem Optimize an input to maximize the activation of a specific neuron or neuron layer.
        \item Saliency Maps
        \subitem Visualize Neuron sensitivity by highlighting inputs that most affect a particular neuron or output~\cite{hsu2023explainable}.
        \item Layer-wise Relevance Propagation
        \subitem Similar to Saliency Maps, decomposes the prediction of a network back to the individual input features~\cite{jia2022interpreting}.
        \item Self-Attention Analysis (LLMs)
        \subitem Sensitivity is represented by attention weights associated with an attention head, which dictates the degree of influence of each token in a sequence model~\cite{shi2021sparsebert}.
        \item Integrated Gradients
        \subitem Quantifies feature importance by integrating gradients along the path from a baseline input to the actual input~\cite{sundararajan2017axiomatic}.
        \item SHAP Values (SHapley Additive exPlanation)
        \subitem Quantifies sensitivity by assigning a contribution score to each input feature based on its impact on the modelâ€™s output~\cite{nohara2022explanation}.
        \item Mechanistic Interpretability
        \subitem Maps out neural circuits in order to determine which parts of a model are responsible for specific capabilities (e.g.\ arithmetic, semantic relationships between words)~\cite{singh2024rethinking}.
    \end{itemize}

    During this initial review, it became apparent that the literature detailing algorithms for determining component
    ``importance'' in Deep Neural Networks is generally coupled with specific optimization techniques, making understanding the current state
    of determining neuron sensitivity for LLMs difficult.
    A sub-goal of this project is to aggregate these techniques in a single document.

    \subsection*{Tool Familiarization}
    Since writing the proposal, I've begun work on gaining familiarity with TensorFlow~\cite{tensorflow_2024} and
    Pytorch\cite{pytorch_2024} by writing a simple ``OR'' gate training implementation using both
    frameworks~\cite{seely_tensorflow_2024,seely2024perceptron}.

    \section*{Challenges and Issues}
    Even with a simple ``OR'' gate, it becomes evident that hyperparameter selection is incredibly important to make
    sure the training can converge on the desired predictions.
    For example, in TensorFlow, when using sigmoid as the activation function, setting ``class weights'' for the single
    negative ``OR'' case helps the model overall converge faster:

    \begin{lstlisting}[caption=Without class weights]
        model.fit(inputs, expected_outputs, epochs=100)

        # Input     Expected	Output
        # [0. 0.]   [0.]        [0.60341406]
        # [0. 1.]   [1.]        [0.74511236]
        # [1. 0.]   [1.]        [0.80108136]
        # [1. 1.]   [1.]        [0.88554966]
    \end{lstlisting}

    \begin{lstlisting}[caption=With class weights]
        class_weight = {0: 3, 1: 1}
        model.fit(inputs, expected_outputs, epochs=100, class_weight=class_weight)

        # Input	    Expected	Output
        # [0. 0.]	[0.]	    [0.26747793]
        # [0. 1.]	[1.]	    [0.67236865]
        # [1. 0.]	[1.]	    [0.7458403]
        # [1. 1.]	[1.]	    [0.94283354]
    \end{lstlisting}

    More experience with TensorFlow and PyTorch is needed.

%    \section*{Next Steps}
%    \begin{enumerate}
%        \item Build multi-layer ANNs with TensorFlow and PyTorch to gain more experience.
%        \item Experiment with visualizing neuron sensitivity in these networks using TensorBoard\cite{tensorboard_2024}.
%        \item Gain a greater fundamental understanding of ANNs and ANN optimization by studying ``Numerical Computation.''
%        and ``Machine Learning Basics'' in literature such as *Deep Learning by Ian Goodfellow et al.*\cite{Goodfellow-et-al-2016}.
%    \end{enumerate}
%
%    \section*{Questions or Feedback Needed}
%    I could use feedback on my next steps listed above - am I focusing on the right action items to properly build my
%    understanding of neuron sensitivity?
%
    \bibliographystyle{plainurl}
    \bibliography{bibliography}
%    \thanks Thanks to ChatGPT~\cite{chatgpt_2024} for helping with my initial forays into PyTorch and TensorFlow, generating
%    occasionally correct BibTeX citations for websites, and helping me with LaTeX syntax.

\end{document}
