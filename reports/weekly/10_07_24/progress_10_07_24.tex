\documentclass{article}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{graphicx}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{enumitem}

\lstset{
    language=Python,
    basicstyle=\ttfamily\footnotesize\selectfont, % use the selected monospaced font
    backgroundcolor=\color{white},
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=10pt,
    frame=single,
    breaklines=true,
    captionpos=b,
    tabsize=4
}

\title{Progress Report - Week 4 \\
\large Novel Methods for Determining Neuron Sensitivity in Large Language Models}
\author{
    [Welby Seely] \\
    \texttt{[wseely@emich.edu]}
}
\date{\today}

\begin{document}

    \maketitle


    \section{Project Overview}\label{sec:project-overview}
    Large Language Model (LLM) power consumption is increasingly problematic for datacenters, with \textit{The Electric Power
    Research Institute} forecasting that data centers may see their electricity consumption double by
    2030~\cite{kindig2024}.

    Optimizing these models is key to reducing power consumption.
    The cornerstone of optimization is determining what you need, and what you don't: determine the importance of
    aspects and components of the system, enabling you to perform systematic optimization with certainty.
    In Artificial Neural Networks (ANN), the atomic unit of these components is the artificial neuron.
    To optimize an ANN, understanding the sensitivity of the neuron to changes in its input and parameters is paramount,
    but the sheer size and complexity of LLMs makes this incredibly challenging.

    Thus, the goal of this research is to:
    \begin{enumerate}
        \item Understand current methodologies for determining neuron sensitivity in LLMs.
        \item Explore improvements that can be made to more accurately and more efficiently determine neuron sensitivity in
        LLMs.
    \end{enumerate}

    The intent is that this research will be a first step towards innovating and improving optimizations for LLMs,
    reducing power consumption and enabling the models to run on lower powered hardware.


    \section{Current Progress}\label{sec:current-progress}

    \subsection{Pruning with LLM--Pruner}\label{subsec:pruning-with-llm--pruner}
    Updated my fork of the LLM-Pruner codebase to save the pruned LLM model using PyTorch.
    Successfully pruned LLM using CPU while evaluating the LLM using GPU using the Llama 2--7B (Decapoda Research)~\cite{decapoda-llama-7B} model.

    Small changes made so far to the codebase can be found on GitHub ~\cite{llm-pruner-pr}.

    \subsection{Measuring Power Consumption Programmatically}\label{subsec:measuring-power-consumption-locally}
    I've continued working on a test harness for measuring the energy usage LLMs. In particular, I've made two innovations:

    \begin{enumerate}
        \item I've further generalized the code performance measurement code - it now measures performance of an LLM using an Aspect Orient Programming (AOP) style.
        This allows the energy and temporal aspects of an evaluation to be recorded without any knowledge of the implementation using a simple annotation on the ``evaluation()'' method of the implementation class.
        \item This aspect can operate on both Huggingface transformer models and raw LLM Pytorch models.
        This is important because LLM pruning projects do not necessarily output their models using the Huggingface transformer format - this requires generating a new model config with updated parameters for all layers.
        Research projects will instead sometimes output their models as raw pytorch checkpoints, which requires an entirely different loading style.
        Importantly, the custom modules of the pruned LLMs in this format must be imported into the classpath.
    \end{enumerate}

    Energy consumption continues to be measured using the pynvml library, which is supported on Volta or newer Nvidia GPUs. This facilitates measuring energy savings on approximated models even when the associated papers do not measure energy savings.

    I've updated the metrics with LLM-Pruner pruner metrics, continuing to use the first 20 rows of the test dataset of ``wikitext-2-v1'' from HuggingFace~\cite{merity2016pointersentinelmixturemodels} as I refine my energy measurement process.
    \\
    The updated code can be found on GitHub~\cite{llm-test-runner}.
    I will also provide links in the description of the submission.


    Updated LLM metrics:
    \begin{itemize}
        \item Baseline: Llama 3--8B
        \begin{itemize}
            \item Total Execution Time: 98.37 s
            \item Total Energy Usage: 26554.41 j
            \item Average Time Per Token: 15.63 ms
            \item Average Energy Per Token: 4.22 j
        \end{itemize}
        \item Pruned: Llama--3--Minitron--4B--Width-Base
        \begin{itemize}
            \item Total Execution Time: 105.82 s
            \item Total Energy Usage: 21235.43 J
            \item Average Time Per Token: 16.28 ms
            \item Average Energy Per Token: 3.27 J
        \end{itemize}
        \item Baseline: Llama 2--7B (Decapoda Research)~\cite{decapoda-llama-7B}
        \begin{itemize}
            \item Total Execution Time: 147.84 S
            \item Total Energy Usage: 40133.24 J
            \item Average Time Per Token: 14.78 ms
            \item Average Energy Per Token: 4.01 J
        \end{itemize}
        \item Pruned with LLM--Pruner~\cite{ma2023llm}: Llama 2--7B (Decapoda Research)
        \begin{itemize}
            \item Total Execution Time: 143.12 s
            \item Total Energy Usage: 30289.60 j
            \item Average Time Per Token: 17.75 ms
            \item Average Energy Per Token: 3.76 j
        \end{itemize}
    \end{itemize}

    \subsection{Comparative Analysis of Neuron Sensitivity Techniques}\label{subsec:comparative-analysis-of-neuron-sensitivity-techniques}

    As a part of literature review, I've grouped different works into particular categories of importance determination.
    I've also added Nvidia's Minitron~\cite{sreenivas2024llm} to the ``Structure Importance Analysis'' section.

    \input{structure-analysis}
    \input{input-sensitivity}
    \input{token-importance}


    \section{Challenges and Issues}\label{sec:challenges-and-issues}
    Follow up to the issue from last report: Was able to prune Llama2 using LLM-Pruner~\cite{llm-pruner-pr} using CPU only due to the low number of operations due to the pruning methodology: only neurons that have a single input and output are pruned.
    64 GB of system RAM were plenty to complete the operation.

    The issues with pruning using a 4090 GPU (24 GB VRAM) should still be solvable using proper memory management.

    \section{Next Steps}\label{sec:next-steps}
    Will continue working on evaluating additional LLM models and optimization techniques, evaluating power usage and importance determination techniques.
    With the improvements made to metrics capture, this will be much easier next week.
    \\
    Structured analysis of importance continues to be of particular interest.


    \section{Questions or Feedback Needed}\label{sec:questions-or-feedback-needed}
    Based on your feedback, I've continuing to work on analyzing energy savings and digging into running the models and approximation code.
    Now that I've been able to measure pruned models in the LLM-Pruner codebase, I will move on to other models ~\cite{ma2023llm}.
    \\
    As per feedback, and time-permitting, I intend to measure power savings when combining optimization techniques described in the literature.
    \bibliographystyle{plainurl}
    \bibliography{bibliography}
    \thanks Thanks to ChatGPT~\cite{chatgpt_2024} for help with PyTorch and for helping generate BibTeX citations for websites.

\end{document}
