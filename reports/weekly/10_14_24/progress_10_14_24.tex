\documentclass{article}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{graphicx}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{enumitem}

\lstset{
    language=Python,
    basicstyle=\ttfamily\footnotesize\selectfont, % use the selected monospaced font
    backgroundcolor=\color{white},
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=10pt,
    frame=single,
    breaklines=true,
    captionpos=b,
    tabsize=4
}

\title{Progress Report - Week 5 \\
\large Novel Methods for Determining Neuron Sensitivity in Large Language Models}
\author{
    [Welby Seely] \\
    \texttt{[wseely@emich.edu]}
}
\date{\today}

\begin{document}

    \maketitle


    \section{Project Overview}\label{sec:project-overview}
    Large Language Model (LLM) power consumption is increasingly problematic for datacenters, with \textit{The Electric Power
    Research Institute} forecasting that data centers may see their electricity consumption double by
    2030~\cite{kindig2024}.

    Optimizing these models is key to reducing power consumption.
    The cornerstone of optimization is determining what you need, and what you don't: determine the importance of
    aspects and components of the system, enabling you to perform systematic optimization with certainty.
    In Artificial Neural Networks (ANN), the atomic unit of these components is the artificial neuron.
    To optimize an ANN, understanding the sensitivity of the neuron to changes in its input and parameters is paramount,
    but the sheer size and complexity of LLMs makes this incredibly challenging.

    Thus, the goal of this research is to:
    \begin{enumerate}
        \item Understand current methodologies for determining neuron sensitivity in LLMs.
        \item Explore improvements that can be made to more accurately and more efficiently determine neuron sensitivity in
        LLMs.
    \end{enumerate}

    The intent is that this research will be a first step towards innovating and improving optimizations for LLMs,
    reducing power consumption and enabling the models to run on lower powered hardware.


    \section{Current Progress}\label{sec:current-progress}
    I've continued working on the test harness for capturing LLM runtime metrics.
    In particular, I've made two updates:

    \subsection{Test Harness Improvement - Energy Measurement Correction}\label{subsec:test-harness-correction}

    Energy measurements were not counting the initial tokens that have to be evaluated before generating additional tokens.
    This has been corrected.

    \subsection{Test Harness Improvement - Perplexity Metric}\label{subsec:perplexity}
    Implemented an additional metric, the average perplexity per token.
    Perplexity is a measurement of the ``uniformity'' or ``certainty'' of token prediction.
    It is equivalent to the exponentiation of the cross-entropy loss between input tokens and the predicted logits~\cite{jelinek1977perplexity}.
    In order to both better understand the concept and gain fine-grained control over deriving perplexity, I've manually implemented the loss calculation:
    \\
    \begin{lstlisting}[language=Python,label={lst:lstlisting}]
            @capture_loss
            def per_token_losses(self, tokens):
                input_ids = tokens['input_ids']
                attention_mask = tokens['attention_mask']
                with torch.no_grad():
                    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                    logits = outputs.logits
                labels = input_ids.clone() # labels are derived from input
                labels = labels[:, 1:].contiguous() # Drop the first label - it isn't useful as no logits are generated for it
                labels = labels.view(-1)  # Flatten the labels to a vector, [batch_size * labels_sequence_length], in preperation for cross_entroy calculation
                logits = logits[:, :-1].contiguous()  # Last logit has no label to compare with
                logits = logits.view(-1, logits.size(-1))  # Flatten the logits to a matrix [batch_size * sequence_length, vocab_size]
                per_token_loss = functional.cross_entropy(logits, labels, reduction='none') # vector of per token losses
                attention_mask_vector = attention_mask[:, :-1].view(-1).contiguous()
                return per_token_loss * attention_mask_vector # apply the attention mask to remove padding, which can skew perplexity measurements
    \end{lstlisting}

    These losses are then aggregated using the ``@capture\_loss'' annotation as a joinpoint:

    \begin{lstlisting}[language=Python,label={lst:lstlisting2}]
        def capture_loss(func):
            class CaptureLoss:
                def __init__(self, func):
                    self.token_count = 0
                    self.aggregate_loss = 0
                    self.func = func

                def capture(self, *args, **kwargs):
                    token_sequences: tensor = args[1]['input_ids']
                    for sequence in token_sequences:
                        self.token_count += len(sequence) - 1 # can't count first token, is not generated as a part of evaluation
                    token_losses = self.func(*args, **kwargs)
                    self.aggregate_loss += token_losses.sum()
                    perplexity = self.aggregate_loss / self.token_count
                    logger.info(f'Perplexity: {perplexity}')
                    return token_losses

            capture = CaptureLoss(func)
            return lambda *args, **kwargs: capture.capture(*args, **kwargs)
    \end{lstlisting}
    \\ \\
    Importantly, lower perplexity values indicate that logit generation is less ``random''.
    The highest perplexity, nearing the vocabulary size, means that logit distribution is random: a random token would be selected upon sampling.
    \\ \\
    The updated code can be found on GitHub~\cite{llm-test-runner}.
    I will also provide the link in the description of the submission.
    \\ \\
    Updated LLM metrics (now with Perplexity):
    \begin{itemize}
        \item Baseline: Llama 3--8B
        \begin{itemize}
            \item Total Execution Time: 114.34 s
            \item Total Energy Usage: 31453.75 J
            \item Average Time Per Token: 12.04 ms
            \item Average Energy Per Token: 3.31 J
            \item Vocab Size: 128000
            \item Average Perplexity per Token: 2.55
        \end{itemize}
        \item Pruned: Llama--3--Minitron--4B--Width-Base
        \begin{itemize}
            \item Total Execution Time: 123.26 s
            \item Total Energy Usage: 26163.65 J
            \item Average Time Per Token: 11.37 ms
            \item Average Energy Per Token: 2.41 J
            \item Vocab Size: 128000
            \item Perplexity: 2.56
        \end{itemize}
        \item Baseline: Llama 2--7B (Decapoda Research)~\cite{decapoda-llama-7B}
        \begin{itemize}
            \item Total Execution Time: 152.09 s
            \item Total Energy Usage: 34084.31 J
            \item Average Time Per Token: 13.01 ms
            \item Average Energy Per Token: 2.92 J
            \item Perplexity: 2.99
            \item Vocab Size: 32000
        \end{itemize}
        \item Pruned with LLM--Pruner~\cite{ma2023llm}: Llama 2--7B (Decapoda Research)
        \begin{itemize}
            \item Total Execution Time: 143.12 s
            \item Total Energy Usage: 30289.60 j
            \item Average Time Per Token: 17.75 ms
            \item Average Energy Per Token: 3.76 j
        \end{itemize}
    \end{itemize}

    \subsection{Comparative Analysis of Neuron Sensitivity Techniques}\label{subsec:comparative-analysis-of-neuron-sensitivity-techniques}

    As a part of literature review, I've grouped different works into particular categories of importance determination.
    I've also added Nvidia's Minitron~\cite{sreenivas2024llm} to the ``Structure Importance Analysis'' section.

    \input{structure-analysis}
    \input{input-sensitivity}
    \input{token-importance}


    \section{Challenges and Issues}\label{sec:challenges-and-issues}
    Capacity is the main challenging - proper time budgeting will be important in the coming weeks to make sure I make sufficient progress as we near midterm presentations.

    \section{Next Steps}\label{sec:next-steps}
    Will continue working on evaluating additional LLM models and optimization techniques, evaluating power usage and importance determination techniques.
    With the improvements made to metrics capture, this will now include accuracy as well.
    \\
    Structured analysis of importance continues to be of particular interest.


    \section{Questions or Feedback Needed}\label{sec:questions-or-feedback-needed}
    Based on your feedback, I've added perplexity to the LLM test harness/test runner.
    Now that rudimentary measurements of accuracy are in place, I will begin measuring additional models.

     If time-permitting in the coming week, I intend to measure power savings when combining optimization techniques described in the literature.
    \bibliographystyle{plainurl}
    \bibliography{bibliography}
    \thanks Thanks to ChatGPT~\cite{chatgpt_2024} for help with PyTorch and for helping generate BibTeX citations for websites.

\end{document}
